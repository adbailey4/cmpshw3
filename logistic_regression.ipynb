{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# this imports all the functions in logistic regression\n",
    "# you should be able to run this cell at any time to \n",
    "# \"reload\" the functions\n",
    "########################################################\n",
    "\n",
    "from logistic_regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 6179)\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# prepare and load the training data.  this involves\n",
    "# reading in the data and finding the best features\n",
    "########################################################\n",
    "\n",
    "# if cant find stopwords you can download using this:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# init\n",
    "stop_words = stopwords.words('english')\n",
    "train_data = 'train.csv'\n",
    "# test_data = 'test.csv'\n",
    "\n",
    "# interpret data\n",
    "messages, text_labels = read_spam_data(train_data)\n",
    "all_train_data = create_train_data(messages, stop_words)\n",
    "int_labels = create_spam_ham_labels(text_labels, spam=1, ham=0)\n",
    "\n",
    "# get sizes\n",
    "n_messages = len(messages)\n",
    "n_features = all_train_data.shape[1]\n",
    "\n",
    "# make sure everything is still aligned\n",
    "print(all_train_data.shape)\n",
    "assert all_train_data.shape[0] == len(messages)\n",
    "assert all_train_data.shape[0] == len(int_labels)\n",
    "assert all_train_data.shape[0] == len(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDAS:\n",
      "\tbase: 8   log_min: -4   log_max: 1\n",
      "\t[0.000244140625, 0.001953125, 0.015625, 0.125, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# this is the definition of the hyper parameters for\n",
    "# the regression\n",
    "########################################################\n",
    "\n",
    "# lambda\n",
    "lambda_base = 8 #np.e\n",
    "lambda_exp_min = -4\n",
    "lambda_exp_max = 1\n",
    "list_of_lambdas = [lambda_base**i for i in range(lambda_exp_min,lambda_exp_max+1)] \n",
    "print(\"LAMBDAS:\\n\\tbase: {}   log_min: {}   log_max: {}\\n\\t{}\".format( \n",
    "      lambda_base, lambda_exp_min, lambda_exp_max, list_of_lambdas))\n",
    "\n",
    "# sigmoid params\n",
    "eta_0 = 0.1\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# divide the data for 10-fold cross validation\n",
    "########################################################\n",
    "\n",
    "# identifiers\n",
    "TRAIN_DATA = \"t_data\"\n",
    "TRAIN_LABELS = \"t_labels\"\n",
    "VALIDATE_DATA = \"v_data\"\n",
    "VALIDATE_LABELS = \"v_labels\"\n",
    "\n",
    "# prep\n",
    "number_of_buckets = 10\n",
    "size_of_bucket = int(n_messages / number_of_buckets)\n",
    "all_train_buckets = dict()\n",
    "\n",
    "# divide into buckets\n",
    "idx = 0\n",
    "for b in range(number_of_buckets):\n",
    "    data = all_train_data[idx:idx+size_of_bucket]\n",
    "    labels = int_labels[idx:idx+size_of_bucket]\n",
    "    all_train_buckets[b] = [data, labels]\n",
    "    idx += size_of_bucket\n",
    "    \n",
    "    \n",
    "# how to create train and validation data sets\n",
    "def get_train_data_set(idx):\n",
    "    t_data, t_labels = list(), list()\n",
    "    v_data, v_labels = None, None\n",
    "    for k in all_train_buckets.keys():\n",
    "        v = all_train_buckets[k]\n",
    "        if k == idx:\n",
    "            v_data = v[0]\n",
    "            v_labels = v[1]\n",
    "        else:\n",
    "            t_data.append(v[0])\n",
    "            t_labels.append(v[1])\n",
    "    return {\n",
    "        TRAIN_DATA: np.vstack(t_data), \n",
    "        TRAIN_LABELS: np.hstack(t_labels),  \n",
    "        VALIDATE_DATA: v_data, \n",
    "        VALIDATE_LABELS: v_labels\n",
    "    }\n",
    "\n",
    "# get data\n",
    "all_training_datasets = [get_train_data_set(x) for x in list(range(number_of_buckets))]\n",
    "\n",
    "# validation\n",
    "assert len(all_training_datasets) == number_of_buckets\n",
    "for ds in all_training_datasets:\n",
    "    assert ds[TRAIN_DATA].shape[1] == n_features\n",
    "    assert ds[TRAIN_DATA].shape[0] == len(ds[TRAIN_LABELS])\n",
    "    assert ds[VALIDATE_DATA].shape[1] == n_features\n",
    "    assert ds[VALIDATE_DATA].shape[0] == len(ds[VALIDATE_LABELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# definition of our regression function\n",
    "########################################################\n",
    "\n",
    "\n",
    "def run_regression(lamda, train, train_labels, validate, validate_labels, \n",
    "                   eta_0=0.1, alpha=0.9, iterations=321, verbose=False):\n",
    "    # init\n",
    "    report_frequency = int(iterations / 16.0)\n",
    "    t = None\n",
    "    \n",
    "    #run regression\n",
    "    try:\n",
    "        weights = np.random.normal(0, 0.2, n_features)\n",
    "        low_val_loss = sys.maxsize\n",
    "        for t in range(iterations):\n",
    "            weights = logistic_regression(train, train_labels, weights, lamda, alpha, eta_0, t)\n",
    "            val_loss = square_loss(validate, validate_labels, weights=weights)\n",
    "            train_loss = square_loss(train, train_labels, weights=weights)\n",
    "            if verbose and t % report_frequency == 0:\n",
    "                print(\"{}:\\t#{}\\ttrain {}  \\t\\tvalidate {}\".format(l, t, train_loss, val_loss))\n",
    "            if val_loss < low_val_loss:\n",
    "                low_val_loss = val_loss\n",
    "    except Exception as e:\n",
    "        print(\"\\nlambda {} #{}: {}\".format(l, t, e), sys.stderr)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # return best\n",
    "    return low_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAMBDA: 0.000244140625\n",
      "\t..........\n",
      "\terrors:    [3.7355176893339728, 5.269783583164283, 4.558848168374628, 4.469990543346688, 5.077990762524534, 4.987165129508705, 3.0488114855260116, 3.410443527928658, 3.2449748800933036, 2.6816344710054514]\n",
      "\terror avg: 4.048516024080623\n",
      "\terror std: 0.8882010854248966\n",
      "\n",
      "LAMBDA: 0.001953125\n",
      "\t..........\n",
      "\terrors:    [3.825441291364687, 5.349313735793666, 4.450388432297149, 4.3081310495184475, 5.086269106955666, 4.941075156542861, 3.135591986157624, 3.4347068019070974, 3.454821895318585, 2.7119899727013848]\n",
      "\terror avg: 4.0697729428557174\n",
      "\terror std: 0.8478667219485675\n",
      "\n",
      "LAMBDA: 0.015625\n",
      "\t..........\n",
      "\terrors:    [3.4201302163685106, 4.841267378780757, 4.659595267762334, 4.482733399099036, 5.003941121606217, 4.845880996699068, 3.482296261424133, 2.6593485288962055, 1.972034617212043, 2.8702568342404122]\n",
      "\terror avg: 3.823748462208872\n",
      "\terror std: 1.0285509395842205\n",
      "\n",
      "LAMBDA: 0.125\n",
      "\t..........\n",
      "\terrors:    [5.3748025811574704, 5.525629315928193, 5.74515952529075, 5.234841444380736, 5.894132724130992, 5.903443546097615, 4.102546836775094, 5.2178982637424465, 4.374454641602058, 4.253591693934563]\n",
      "\terror avg: 5.162650057303992\n",
      "\terror std: 0.6465016412178921\n",
      "\n",
      "LAMBDA: 1\n",
      "\t..........\n",
      "\terrors:    [11.344750028365475, 11.185253515964904, 10.464070001036706, 10.060072869262164, 10.592200062081446, 10.732119467268232, 9.960952505481286, 10.013542172417313, 10.07058565558946, 9.162786745713106]\n",
      "\terror avg: 10.358633302318008\n",
      "\terror std: 0.6115787838976682\n",
      "\n",
      "LAMBDA: 8\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ucsc_projects\\classes\\cmps242\\hw2\\cmpshw3\\logistic_regression.py:140: RuntimeWarning: overflow encountered in multiply\n",
      "  new_weights = weights*(1-(eta*l)) - (eta*np.matmul(np.array(y_hat-labels), inputs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "\terrors:    [20.586098017995305, 19.729096328849796, 17.862322939955888, 17.475216469670702, 19.552802565609475, 18.682246593415172, 16.659846953034688, 18.852758454994675, 20.13153334733673, 16.0]\n",
      "\terror avg: 18.553192167086245\n",
      "\terror std: 1.4441299064518551\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# run on each of our k-folded datasets\n",
    "########################################################\n",
    "\n",
    "# prep\n",
    "lambda_to_errors = dict()\n",
    "\n",
    "# calculate for our lambdas\n",
    "for l in list_of_lambdas:\n",
    "    print(\"\\nLAMBDA: {}\\n\\t\".format(l),end='')\n",
    "    errors = list()\n",
    "    for dataset in all_training_datasets:\n",
    "        e = run_regression(l, dataset[TRAIN_DATA], dataset[TRAIN_LABELS], \n",
    "                       dataset[VALIDATE_DATA], dataset[VALIDATE_LABELS])\n",
    "        errors.append(e)\n",
    "        print('.', end='')\n",
    "    print(\"\\n\\terrors:    {}\".format(errors))\n",
    "    print(\"\\terror avg: {}\".format(np.mean(errors)))\n",
    "    print(\"\\terror std: {}\".format(np.std(errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
