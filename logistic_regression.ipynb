{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was written by Andrew Bailey and Trevor Pesout. We both contributed equally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"Module Docstrings\"\"\"\n",
    "########################################################################\n",
    "# File: template.py\n",
    "#  executable: template.py\n",
    "#\n",
    "# Author: Andrew Bailey/ Trevor Pesout\n",
    "# History: Created 10/18/17\n",
    "########################################################################\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def sigmoid(x, alpha=1, beta=0):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "\n",
    "    # this can overflow a double, so we use different implementations\n",
    "    # return math.exp(alpha*(x+beta)) / (1 + math.exp(alpha*(x+beta)))\n",
    "    if x < 0:\n",
    "        return 1 - 1 / (1 + math.exp(alpha*(x+beta)))\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-alpha*(x+beta)))\n",
    "\n",
    "\n",
    "\n",
    "def integral_of_sigmoid(x):\n",
    "    \"\"\"Returns the value from the integral of the sigmoid function\"\"\"\n",
    "    return np.log((1+np.exp(x)))\n",
    "\n",
    "\n",
    "def iterative_reweighted_least_squares(inputs, labels, weights):\n",
    "    \"\"\"Newton-Raphson iterative reweighted least squares\"\"\"\n",
    "\n",
    "\n",
    "def logistic_hessian(inputs, weights):\n",
    "    \"\"\"Calculate hessian matrix for logistic regression\"\"\"\n",
    "    y_hat = logistic_evaluation(inputs, weights)\n",
    "    b = np.array([1-y for y in y_hat])\n",
    "    # p(1-p)\n",
    "\n",
    "    prob = [y_hat*b]\n",
    "    n = len(prob[0])\n",
    "    assert n == len(y_hat)\n",
    "    d = np.repeat(prob, [n,], axis=0)\n",
    "    I = np.identity(n)\n",
    "    R = np.multiply(I, prob)\n",
    "    # hessian = np.matmul(np.matmul(inputs.T, R), inputs)\n",
    "    return R\n",
    "\n",
    "def read_spam_data(csv_f):\n",
    "    \"\"\"Read in csv of ham spam data and return two lists of data\"\"\"\n",
    "    label = []\n",
    "    message = []\n",
    "    with open(csv_f) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            message.append(row['sms'])\n",
    "            label.append(row['label'])\n",
    "            # print(row['label'], row['sms'])\n",
    "    return message, label\n",
    "\n",
    "\n",
    "#TODO tpesout: make this persistent, so we can use it with the test data.\n",
    "# we want to make this use the same features with different data (so we can test)\n",
    "def create_train_data(corpus, stop_words, bigram=False, lowercase=True):\n",
    "    \"\"\"Extract features from corpus and perform the tf-idf term weighting as well as removing stopwords\"\"\"\n",
    "    # option to have 'this word' as a feature along with 'this' and 'word'\n",
    "    if bigram:\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b',\n",
    "                                     min_df=1, decode_error='ignore', stop_words=stop_words,\n",
    "                                     lowercase=lowercase)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(decode_error='ignore', stop_words=stop_words, lowercase=lowercase)\n",
    "\n",
    "    # create feature vector\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    transformer = TfidfTransformer(smooth_idf=False, norm='l2')\n",
    "    # normalize and perform tf-idf\n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    return tfidf.toarray()\n",
    "\n",
    "\n",
    "def create_spam_ham_labels(labels_text, spam=1, ham=0):\n",
    "    \"\"\"Convert spam and ham into either 0 or 1 \"\"\"\n",
    "    integer_labels = []\n",
    "    for label in labels_text:\n",
    "        if label == \"spam\":\n",
    "            integer_labels.append(spam)\n",
    "        else:\n",
    "            assert label == \"ham\"\n",
    "            integer_labels.append(ham)\n",
    "    return integer_labels\n",
    "\n",
    "\n",
    "def square_loss(features, labels, weights):\n",
    "    \"\"\"Calculates total loss of lists of features and labels given weights\"\"\"\n",
    "    total_loss = 0\n",
    "    for index, feature in enumerate(features):\n",
    "        a = sigmoid(np.matmul(weights, feature.T))\n",
    "        total_loss += ((a-labels[index])**2) / 2.0\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def logistic_loss(features, labels, weights):\n",
    "    \"\"\"Calculates total loss of lists of features and labels given weights\"\"\"\n",
    "    total_loss = 0\n",
    "    for index, feature in enumerate(features):\n",
    "        y = labels[index]\n",
    "        a = np.matmul(weights, feature.T)\n",
    "        y_hat = sigmoid(a)\n",
    "        if y_hat == 0:\n",
    "            print(\"ERROR its 0\")\n",
    "        # need to double check this equation\n",
    "        total_loss += y*np.log(y) + (1-y)*np.log(1-y_hat)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def logistic_evaluation(inputs, weights):\n",
    "    \"\"\"Returns predicted outputs from the inputs and weights\"\"\"\n",
    "    prediction = []\n",
    "    for index, feature in enumerate(inputs):\n",
    "        a = np.matmul(weights, feature.T)\n",
    "        y_hat = sigmoid(a)\n",
    "        prediction.append(y_hat)\n",
    "    return np.array(prediction)\n",
    "\n",
    "\n",
    "def logistic_regression(inputs, labels, weights, l, alpha, eta_0, step):\n",
    "    \"\"\"Implement regularized logistic regression with moving learning weight\"\"\"\n",
    "    eta = eta_0*(step**alpha)\n",
    "    y_hat = logistic_evaluation(inputs, weights)\n",
    "    new_weights = weights*(1-(eta*l)) - (eta*np.matmul(np.array(y_hat-labels), inputs))\n",
    "    return new_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 6117)\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# prepare and load the training data.  this involves\n",
    "# reading in the data and finding the best features\n",
    "########################################################\n",
    "\n",
    "# if cant find stopwords you can download using this:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# init\n",
    "stop_words = stopwords.words('english')\n",
    "train_data = 'train.csv'\n",
    "# test_data = 'test.csv'\n",
    "\n",
    "# interpret data\n",
    "messages, text_labels = read_spam_data(train_data)\n",
    "all_train_data = create_train_data(messages, stop_words)\n",
    "int_labels = create_spam_ham_labels(text_labels, spam=1, ham=0)\n",
    "\n",
    "# get sizes\n",
    "n_messages = len(messages)\n",
    "n_features = all_train_data.shape[1]\n",
    "# make sure everything is still aligned\n",
    "print(all_train_data.shape)\n",
    "assert all_train_data.shape[0] == len(messages)\n",
    "assert all_train_data.shape[0] == len(int_labels)\n",
    "assert all_train_data.shape[0] == len(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDAS:\n",
      "\tbase: 2   log_min: -1   log_max: 3\n",
      "\t[0.5, 1, 2, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# this is the definition of the hyper parameters for\n",
    "# the regression\n",
    "########################################################\n",
    "\n",
    "# lambda\n",
    "lambda_base = 2 #8 #np.e\n",
    "lambda_exp_min = -1 #-5\n",
    "lambda_exp_max = 3 #1\n",
    "list_of_lambdas = [lambda_base**i for i in range(lambda_exp_min,lambda_exp_max+1)] \n",
    "print(\"LAMBDAS:\\n\\tbase: {}   log_min: {}   log_max: {}\\n\\t{}\".format( \n",
    "      lambda_base, lambda_exp_min, lambda_exp_max, list_of_lambdas))\n",
    "\n",
    "# sigmoid params\n",
    "eta_0 = 0.1\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# divide the data for 10-fold cross validation\n",
    "########################################################\n",
    "\n",
    "# identifiers\n",
    "TRAIN_DATA = \"t_data\"\n",
    "TRAIN_LABELS = \"t_labels\"\n",
    "VALIDATE_DATA = \"v_data\"\n",
    "VALIDATE_LABELS = \"v_labels\"\n",
    "\n",
    "# prep\n",
    "number_of_buckets = 10\n",
    "size_of_bucket = int(n_messages / number_of_buckets)\n",
    "all_train_buckets = dict()\n",
    "\n",
    "# divide into buckets\n",
    "idx = 0\n",
    "for b in range(number_of_buckets):\n",
    "    data = all_train_data[idx:idx+size_of_bucket]\n",
    "    labels = int_labels[idx:idx+size_of_bucket]\n",
    "    all_train_buckets[b] = [data, labels]\n",
    "    idx += size_of_bucket\n",
    "    \n",
    "    \n",
    "# how to create train and validation data sets\n",
    "def get_train_data_set(idx):\n",
    "    t_data, t_labels = list(), list()\n",
    "    v_data, v_labels = None, None\n",
    "    for k in all_train_buckets.keys():\n",
    "        v = all_train_buckets[k]\n",
    "        if k == idx:\n",
    "            v_data = v[0]\n",
    "            v_labels = v[1]\n",
    "        else:\n",
    "            t_data.append(v[0])\n",
    "            t_labels.append(v[1])\n",
    "    return {\n",
    "        TRAIN_DATA: np.vstack(t_data), \n",
    "        TRAIN_LABELS: np.hstack(t_labels),  \n",
    "        VALIDATE_DATA: v_data, \n",
    "        VALIDATE_LABELS: v_labels\n",
    "    }\n",
    "\n",
    "# get data\n",
    "all_training_datasets = [get_train_data_set(x) for x in list(range(number_of_buckets))]\n",
    "\n",
    "# validation\n",
    "assert len(all_training_datasets) == number_of_buckets\n",
    "for ds in all_training_datasets:\n",
    "    assert ds[TRAIN_DATA].shape[1] == n_features\n",
    "    assert ds[TRAIN_DATA].shape[0] == len(ds[TRAIN_LABELS])\n",
    "    assert ds[VALIDATE_DATA].shape[1] == n_features\n",
    "    assert ds[VALIDATE_DATA].shape[0] == len(ds[VALIDATE_LABELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# definition of our regression function\n",
    "########################################################\n",
    "\n",
    "\n",
    "def run_regression(lamda, train, train_labels, validate, validate_labels, \n",
    "                   eta_0=0.1, alpha=0.9, iterations=321, verbose=False):\n",
    "    # init\n",
    "    report_frequency = int(iterations / 16.0)\n",
    "    t = None\n",
    "    \n",
    "    #run regression\n",
    "    try:\n",
    "        weights = np.random.normal(0, 0.2, n_features)\n",
    "        min_validate_loss = sys.maxsize\n",
    "        min_train_loss = sys.maxsize\n",
    "        for t in range(iterations):\n",
    "            weights = logistic_regression(train, train_labels, weights, lamda, alpha, eta_0, t)\n",
    "            train_loss = square_loss(train, train_labels, weights=weights)\n",
    "            val_loss = square_loss(validate, validate_labels, weights=weights)\n",
    "            if verbose and t % report_frequency == 0:\n",
    "                print(\"{}:\\t#{}\\ttrain {}  \\t\\tvalidate {}\".format(l, t, train_loss, val_loss))\n",
    "            if val_loss < min_validate_loss: min_validate_loss = val_loss\n",
    "            if train_loss < min_train_loss: min_train_loss = train_loss\n",
    "    except Exception as e:\n",
    "        print(\"\\nlambda {} #{}: {}\".format(l, t, e), sys.stderr)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # return best\n",
    "    return min_validate_loss, min_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 1, 2, 4, 8]\n",
      "\n",
      "LAMBDA: 0.5\n",
      "\t.......... (189s)\n",
      "\tv errors:    [8.722265972335437, 8.851553887473258, 8.161541421397157, 7.8909732646046455, 8.2513509167522, 8.528565748789068, 7.829298577909078, 7.833769793891729, 7.784868023068944, 6.757337983368127]\n",
      "\tv error avg: 8.061152558958964\n",
      "\tv error std: 0.5689656764164073\n",
      "\n",
      "LAMBDA: 1\n",
      "\t.......... (205s)\n",
      "\tv errors:    [11.34994706193415, 11.209667760090866, 10.529339206739653, 10.173748238342034, 10.66077608658416, 10.834337085713807, 10.158075353690114, 10.14953536899959, 10.221142869046135, 9.130514611547145]\n",
      "\tv error avg: 10.441708364268766\n",
      "\tv error std: 0.6024931433456062\n",
      "\n",
      "LAMBDA: 2\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewbailey/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:140: RuntimeWarning: overflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......... (228s)\n",
      "\tv errors:    [14.893598505402327, 14.549990924965686, 13.623481794244631, 13.439084507720395, 14.114380954119417, 14.093482607648834, 13.389368375869012, 13.47353815184741, 13.707439290008221, 12.481803259508302]\n",
      "\tv error avg: 13.776616837133423\n",
      "\tv error std: 0.6419321031885954\n",
      "\n",
      "LAMBDA: 4\n",
      "\t.......... (178s)\n",
      "\tv errors:    [18.894784365058904, 18.27902204610276, 17.8712129397925, 17.46786106934318, 17.941391448334166, 18.005974929368932, 16.224220106448893, 17.20025567738062, 17.5810437066695, 16.266852424886654]\n",
      "\tv error avg: 17.57326187133861\n",
      "\tv error std: 0.7958000234281607\n",
      "\n",
      "LAMBDA: 8\n",
      "\t.......... (173s)\n",
      "\tv errors:    [20.548131675990238, 19.81448021556645, 18.0561372342424, 17.764334775620107, 19.515614257448004, 19.06745448478471, 16.89772426538649, 18.987700972501973, 19.91704444830481, 16.0]\n",
      "\tv error avg: 18.656862232984516\n",
      "\tv error std: 1.3722788755987347\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# run on each of our k-folded datasets\n",
    "########################################################\n",
    "\n",
    "# prep\n",
    "lambda_to_validate_errors = dict()\n",
    "lambda_to_training_errors = dict()\n",
    "\n",
    "print(list_of_lambdas)\n",
    "# calculate for our lambdas\n",
    "for l in list_of_lambdas:\n",
    "    start = timer()\n",
    "    print(\"\\nLAMBDA: {}\\n\\t\".format(l),end='')\n",
    "    v_errors = list()\n",
    "    t_errors = list()\n",
    "    lambda_to_validate_errors[l] = v_errors\n",
    "    lambda_to_training_errors[l] = t_errors\n",
    "    for dataset in all_training_datasets:\n",
    "        v_error, t_error = run_regression(l, dataset[TRAIN_DATA], dataset[TRAIN_LABELS], \n",
    "                       dataset[VALIDATE_DATA], dataset[VALIDATE_LABELS])\n",
    "        v_errors.append(v_error)\n",
    "        t_errors.append(t_error)\n",
    "        print('.', end='')\n",
    "    print(\" ({}s)\".format(int(timer() - start)))\n",
    "    print(\"\\tv errors:    {}\".format(v_errors))\n",
    "    print(\"\\tv error avg: {}\".format(np.mean(v_errors)))\n",
    "    print(\"\\tv error std: {}\".format(np.std(v_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCVKICIikhIlEBERScn/A0GslmU/pchkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114dfe470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safds\n",
      "safds\n",
      "safds\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# analyze and print plots of the data\n",
    "########################################################\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_plot_data(items):\n",
    "    items.sort(key=lambda x: x[0])\n",
    "    x, y, var, log_lambda = list(), list(), list(), lambda_exp_min\n",
    "    for k, v in items:\n",
    "        x.append(log_lambda)\n",
    "        y.append(np.mean(v))\n",
    "        var.append(np.std(v)**2)\n",
    "        assert lambda_base**log_lambda == k\n",
    "        log_lambda += 1\n",
    "    return x, y, var\n",
    "\n",
    "validate_x, validate_y, validate_v = get_plot_data(list(lambda_to_validate_errors.items()))\n",
    "train_x, train_y, train_v = get_plot_data(list(lambda_to_training_errors.items()))\n",
    "\n",
    "plt.errorbar(train_x, train_y, train_v, None, 'bo-', label=\"Train\")\n",
    "plt.errorbar(validate_x, validate_y, validate_v, None, 'ro-', label=\"Validate\")\n",
    "plt.axis([lambda_exp_min-.5, lambda_exp_max-.5, -2, 20])\n",
    "plt.legend(bbox_to_anchor=(.31,.95))\n",
    "plt.title(\"Logistic Regression Error Rates\")\n",
    "plt.xlabel(\"Log of Lambda (Base {})\".format(lambda_base))\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham vs Spam: Logistic Regression\n",
    "\n",
    "##### Andrew Bailey and Trevor Pesout\n",
    "\n",
    "The task at hand is to classify email headers as either 'spam' or 'ham'. The assumtion is that there are some key words that are in the spam email headers that give them away as spam and by using logistic regression we should be able to classify the difference betwen spam and ham based on the header. \n",
    "\n",
    " ##### _examples:_ \n",
    " \n",
    "spam - T-Mobile customer you may now claim your FREE CAMERA PHONE upgrade & a pay & go sim card for your loyalty. Call on 0845 021 3680.Offer ends 28thFeb.T&C's apply  \n",
    "\n",
    "\n",
    "ham  -  Thanks for this hope you had a good day today   \n",
    "\n",
    "### Natural Language Processing \n",
    "\n",
    "\n",
    "We first needed to convert the phrases from the headers of theses emails into a format which is readable by the logistic regression algorithm. \n",
    "\n",
    "Some words do not add information such as 'the' and should be removed. Therefore we used the very convenient CountVectorizer class to remove stopwords from all sentences. CountVectorizer also creates an index for each word and vectorizes each input sentence into a count vector. The count vector represents every word in the sentence but it is now in a fixed length vector. In order to normalize the count vector we used TfidfTransformer to weight words by the frequency of their appearance within the entire document. \n",
    "\n",
    "\n",
    "$tf(t,d) = $ number of times a term t occurs in document d.  \n",
    "$n_d$ is the number of documents and $df(d,t)$ is how many documents have term t.\n",
    "$idf(t,d) = log \\frac{n_d}{df(d,t)}+1$ number of times a term t occurs in document d.  \n",
    "$tfidf(t,d) = tf(t,d)*idf(t,d)$\n",
    "\n",
    "These counts $tfidf(t,d)$ are then normalized by the Euclidean norm.\n",
    "\n",
    "Now that we have encoded the sentence information into a vector, we convert the ham and spam labels to 0 and 1 respectively in order to classify each sentence using the logistic regression algorithm.\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Now that the inputs have been vectorized, the logistic regression algorithm can be used to determine an optimized weight vector. \n",
    "\n",
    "Each sentence is represented by $X = [x_1, x_2, ... x_m]$  \n",
    "The label of spam or ham is represented by $y \\in (0,1)$  \n",
    "The weight vector is $\\mathbf{w} =  [w_1, w_2, ... w_m]$  \n",
    "\n",
    "The output before the sigmoid function is defined as $a_i = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=0}^{m} w_j x_j= \\mathbf{w}^TX_i$  \n",
    "\n",
    "Sigmoid function is $\\phi(a) = \\frac{1}{1 + e^{-a}}$  \n",
    "Final output is $\\phi(a_i) = t_i$\n",
    "\n",
    "Likelihood  \n",
    "$L(\\mathbf{w}) = P(\\mathbf{y} \\mid \\mathbf{X};\\mathbf{w})= \\prod_{i=1}^{n} P\\big(y^{(i)} \\mid X^{(i)}; \\mathbf{w}\\big)= \\prod^{n}_{i=1}\\bigg(\\phi\\big(a^{(i)}\\big)\\bigg)^{y^{(i)}} \\bigg(1-\\phi\\big(a^{(i)}\\big)\\bigg)^{1-y^{(i)}}$\n",
    "\n",
    "Regularization Terms:  \n",
    "$L2: \\frac{\\lambda}{2}\\lVert \\mathbf{w} \\lVert_2 = \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$  \n",
    "$L1: \\lambda\\lVert \\mathbf{w} \\lVert_1 = \\lambda \\sum_{j=1}^{m} |w_j|$\n",
    "\n",
    "\n",
    "\n",
    "Objective/Cost Function (Negative Log Likelihood + Regularization)  \n",
    "$E(w_j) = \\sum_{i=1}^{m} \\Big[ - y^{(i)} log \\bigg( \\phi\\big(a^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(a^{(i)}\\big)\\bigg)\\Big] - \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$\n",
    "\n",
    "Minimize Negative Log Likelihood  \n",
    "$\\begin{align}\n",
    "\\nabla E(w_j) = \\frac{\\partial}{\\partial w_j} l(w_j) &= \\sum_{j=1}^{m} \\bigg[\\bigg(y \\frac{1}{\\phi(a)} - (1-y) \\frac{1}{1-\\phi{(a)}} \\bigg) \\frac{\\partial}{\\partial w_j}\\phi(a) - \\frac{\\lambda}{2} \\frac{\\partial}{\\partial w_j} w_j^2\\bigg] \\\\ \n",
    "&=- \\sum_{j=1}^{m} \\big(y - \\phi(z)\\big)x_j - \\lambda w_j\n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "We now have the gradient for the weights and use a decreasing function for the step size $\\eta$ in order to decrease the size of the step with each iteration t.\n",
    "\n",
    "$\\begin{align}\n",
    "\\eta &= \\eta_0 * t^{-\\alpha} \\\\\n",
    "\\Delta{w_j} & = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big) - \\eta \\lambda w_j\n",
    "\\end{align}$\n",
    "\n",
    "We can then turn these gradients into vector representation.\n",
    "$\\begin{align}\n",
    "\\nabla E(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} l(\\mathbf{w}) &=  - \\eta\\big(\\mathbf{y} - \\mathbf{\\phi(z)}\\big)\\mathbf{X}\\big) -\\mathbf{w}\\eta \\lambda \n",
    "\\end{align}$\n",
    "\n",
    "In order to utilize a bias term the input vector for each sample was prepended with a 1 and the weight vector was not regularized on that term. In order to accomplish this we allowed for a bias regularization option which set the regulzarizers to 0 for that term.  \n",
    "\n",
    "\n",
    "### Iterative Least Squares Regression\n",
    "\n",
    "The update for the iterative reweighted least squares is defined as $\\mathbf{w}^{(new)} = \\mathbf{w}^{(old)} - \\mathbf{H}^{-1}\\nabla E(\\mathbf{w})$\n",
    "\n",
    "$\\begin{align}\n",
    "\\nabla E(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} l(\\mathbf{w}) &=  - \\big(\\mathbf{y} - \\mathbf{\\phi(z)}\\big)\\mathbf{X}\\big) -\\mathbf{w}\\lambda \n",
    "\\end{align}$\n",
    "\n",
    "R is an N X N diagonal matrix with elements $R_{nn} = y_n(1-y_n)$.\n",
    "\n",
    "$\\begin{align}\n",
    "H &= \\nabla \\nabla E(\\mathbf{w}) = \\mathbf{X}^T \\mathbf{R} \\mathbf{X}\n",
    "\\end{align}$\n",
    "\n",
    "Therefore the update can be defined as:\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathbf{w}^{(new)} = \\mathbf{w}^{(old)} - (\\mathbf{X}^T \\mathbf{R} \\mathbf{X})^{-1} (- \\big(\\mathbf{y} - \\mathbf{\\phi(z)}\\big)\\mathbf{X}\\big) -\\mathbf{w} \\lambda ))\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "### Results\n",
    "\n",
    "\n",
    "\n",
    "### Analysis\n",
    "\n",
    "The iterative least squares regression algorithm usually requires significantly fewer iterations than the standard logistic regression algorithm. However, the computing the Hessian is very expensive and with a naieve python implementation takes much longer to reach the minimum. Also, computing the Hessian requires a batch of data, so there is no natural online method using iterative least squares regression. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
