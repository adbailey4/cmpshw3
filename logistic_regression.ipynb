{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# this imports all the functions in logistic regression\n",
    "# you should be able to run this cell at any time to \n",
    "# \"reload\" the functions\n",
    "########################################################\n",
    "\n",
    "from logistic_regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 6179)\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# prepare and load the training data.  this involves\n",
    "# reading in the data and finding the best features\n",
    "########################################################\n",
    "\n",
    "# if cant find stopwords you can download using this:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# test_data = 'test.csv'\n",
    "train_data = 'train.csv'\n",
    "\n",
    "messages, text_labels = read_spam_data(train_data)\n",
    "all_train_data = create_train_data(messages, stop_words)\n",
    "int_labels = create_spam_ham_labels(text_labels, spam=1, ham=0)\n",
    "print(all_train_data.shape)\n",
    "# make sure everything is still aligned\n",
    "assert all_train_data.shape[0] == len(messages)\n",
    "assert all_train_data.shape[0] == len(int_labels)\n",
    "assert all_train_data.shape[0] == len(text_labels)\n",
    "n_features = all_train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000244140625, 0.001953125, 0.015625, 0.125, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# this is the definition of the hyper parameters for\n",
    "# the regression\n",
    "########################################################\n",
    "lambda_base = 8 #np.e\n",
    "lambda_exp_min = -4\n",
    "lambda_exp_max = 1\n",
    "\n",
    "eta_0 = 0.1\n",
    "alpha = 0.9\n",
    "# l is the lambda (regularizer)\n",
    "list_of_lambdas = [lambda_base**i for i in range(lambda_exp_min,lambda_exp_max+1)] #np.linspace(0, .07, 5)\n",
    "print(list_of_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# divide the data for 10-fold cross validation\n",
    "########################################################\n",
    "\n",
    "# todo\n",
    "\n",
    "X = all_train_data\n",
    "\n",
    "train = X[:2000]\n",
    "train_labels = int_labels[:2000]\n",
    "val = X[2000:]\n",
    "val_labels = int_labels[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAMBDA: 0.000244140625\n",
      "0.000244140625:\t#0\ttrain 250.48026711675016  \t\tvalidate 126.40091458659214\n",
      "0.000244140625:\t#16\ttrain 2.8068480093914543  \t\tvalidate 13.82272832135285\n",
      "0.000244140625:\t#32\ttrain 0.8384750328269709  \t\tvalidate 11.993340714386788\n",
      "0.000244140625:\t#48\ttrain 0.6768217916261067  \t\tvalidate 11.431372739486017\n",
      "0.000244140625:\t#64\ttrain 0.6444004183551983  \t\tvalidate 11.16495495444631\n",
      "0.000244140625:\t#80\ttrain 0.6343724518098279  \t\tvalidate 11.013565044162604\n",
      "0.000244140625:\t#96\ttrain 0.6303723237392387  \t\tvalidate 10.918903861893746\n",
      "0.000244140625:\t#112\ttrain 0.6284866233204475  \t\tvalidate 10.856349756350882\n",
      "0.000244140625:\t#128\ttrain 0.6274876196739667  \t\tvalidate 10.813646555633442\n",
      "0.000244140625:\t#144\ttrain 0.6269118704887062  \t\tvalidate 10.783958357867306\n",
      "0.000244140625:\t#160\ttrain 0.6265590720886458  \t\tvalidate 10.763134032953237\n",
      "0.000244140625:\t#176\ttrain 0.6263331227333511  \t\tvalidate 10.748480280851402\n",
      "0.000244140625:\t#192\ttrain 0.6261838694430802  \t\tvalidate 10.738157539552892\n",
      "0.000244140625:\t#208\ttrain 0.6260832337930317  \t\tvalidate 10.730861313243118\n",
      "0.000244140625:\t#224\ttrain 0.6260145224107886  \t\tvalidate 10.725644563628864\n",
      "0.000244140625:\t#240\ttrain 0.625967288478994  \t\tvalidate 10.721813347790906\n",
      "0.000244140625:\t#256\ttrain 0.6259347139518013  \t\tvalidate 10.718861474099358\n",
      "0.000244140625:\t#272\ttrain 0.625912207245475  \t\tvalidate 10.716426179236274\n",
      "0.000244140625:\t#288\ttrain 0.6258966142853322  \t\tvalidate 10.714255380700216\n",
      "0.000244140625:\t#304\ttrain 0.6258857498463538  \t\tvalidate 10.712181795724204\n",
      "0.000244140625:\t#320\ttrain 0.6258781015714656  \t\tvalidate 10.710101796153054\n",
      "\n",
      "LAMBDA: 0.001953125\n",
      "0.001953125:\t#0\ttrain 254.64073141571305  \t\tvalidate 126.18786603488114\n",
      "0.001953125:\t#16\ttrain 2.8645242705663785  \t\tvalidate 13.72346025751319\n",
      "0.001953125:\t#32\ttrain 0.8824438596864902  \t\tvalidate 11.96700562101713\n",
      "0.001953125:\t#48\ttrain 0.7088754407386219  \t\tvalidate 11.475312550701645\n",
      "0.001953125:\t#64\ttrain 0.6713855921791438  \t\tvalidate 11.282213250284375\n",
      "0.001953125:\t#80\ttrain 0.6595366964658352  \t\tvalidate 11.201691122350995\n",
      "0.001953125:\t#96\ttrain 0.6552018219070279  \t\tvalidate 11.169474287717795\n",
      "0.001953125:\t#112\ttrain 0.6535507281235848  \t\tvalidate 11.15734793246847\n",
      "0.001953125:\t#128\ttrain 0.6529015845690014  \t\tvalidate 11.152768877597564\n",
      "0.001953125:\t#144\ttrain 0.6526229782602323  \t\tvalidate 11.150757144275738\n",
      "0.001953125:\t#160\ttrain 0.6524867334225551  \t\tvalidate 11.149584531827633\n",
      "0.001953125:\t#176\ttrain 0.6524126896475115  \t\tvalidate 11.148707855933335\n",
      "0.001953125:\t#192\ttrain 0.6523703653857424  \t\tvalidate 11.147972786320626\n",
      "0.001953125:\t#208\ttrain 0.6523458834295546  \t\tvalidate 11.147347170786238\n",
      "0.001953125:\t#224\ttrain 0.6523318082138821  \t\tvalidate 11.146831001365339\n",
      "0.001953125:\t#240\ttrain 0.6523238232015316  \t\tvalidate 11.146424901237513\n",
      "0.001953125:\t#256\ttrain 0.6523193666132021  \t\tvalidate 11.146121496519296\n",
      "0.001953125:\t#272\ttrain 0.6523169230788214  \t\tvalidate 11.14590618793551\n",
      "0.001953125:\t#288\ttrain 0.6523156079054806  \t\tvalidate 11.145760837948375\n",
      "0.001953125:\t#304\ttrain 0.6523149134040712  \t\tvalidate 11.14566733126024\n",
      "0.001953125:\t#320\ttrain 0.6523145537287292  \t\tvalidate 11.145609911811919\n",
      "\n",
      "LAMBDA: 0.015625\n",
      "0.015625:\t#0\ttrain 248.97556450085497  \t\tvalidate 126.22344541605422\n",
      "0.015625:\t#16\ttrain 3.4797057210873765  \t\tvalidate 14.434608068818191\n",
      "0.015625:\t#32\ttrain 1.4482001915769196  \t\tvalidate 13.062064218458026\n",
      "0.015625:\t#48\ttrain 1.3068251890298412  \t\tvalidate 12.858985025403639\n",
      "0.015625:\t#64\ttrain 1.2935555167422412  \t\tvalidate 12.801177657077579\n",
      "0.015625:\t#80\ttrain 1.291796516514218  \t\tvalidate 12.780109200486335\n",
      "0.015625:\t#96\ttrain 1.291532987055743  \t\tvalidate 12.773642087926413\n",
      "0.015625:\t#112\ttrain 1.2914980896319481  \t\tvalidate 12.772091444141228\n",
      "0.015625:\t#128\ttrain 1.2911763255897952  \t\tvalidate 12.77313127295712\n",
      "0.015625:\t#144\ttrain 0.8847825314952923  \t\tvalidate 15.490977231677714\n",
      "0.015625:\t#160\ttrain 2.5901594381534583  \t\tvalidate 12.733956225339245\n",
      "0.015625:\t#176\ttrain 1.1929081217531516  \t\tvalidate 12.222334931630344\n",
      "0.015625:\t#192\ttrain 0.9229630050905544  \t\tvalidate 12.720939778801194\n",
      "0.015625:\t#208\ttrain 0.7800371525673192  \t\tvalidate 14.199636937857786\n",
      "0.015625:\t#224\ttrain 1.1842305424534803  \t\tvalidate 13.231196851773532\n",
      "0.015625:\t#240\ttrain 4.455094855686352  \t\tvalidate 12.760485664313707\n",
      "0.015625:\t#256\ttrain 1.2647103894093166  \t\tvalidate 13.337165964446806\n",
      "0.015625:\t#272\ttrain 1.3247574744394985  \t\tvalidate 13.821358071505562\n",
      "0.015625:\t#288\ttrain 0.8140839737737593  \t\tvalidate 13.620316131766383\n",
      "0.015625:\t#304\ttrain 0.882557056351218  \t\tvalidate 11.545314512056127\n",
      "0.015625:\t#320\ttrain 1.1724191301631668  \t\tvalidate 12.589360814429648\n",
      "\n",
      "LAMBDA: 0.125\n",
      "0.125:\t#0\ttrain 252.260565724478  \t\tvalidate 126.9510568699424\n",
      "0.125:\t#16\ttrain 10.674614489992234  \t\tvalidate 18.3690793388972\n",
      "0.125:\t#32\ttrain 10.18157704295231  \t\tvalidate 18.219771806147058\n",
      "0.125:\t#48\ttrain 36.8345238145821  \t\tvalidate 31.26566308302499\n",
      "0.125:\t#64\ttrain 23.974293187960967  \t\tvalidate 22.281215023472168\n",
      "0.125:\t#80\ttrain 78.64343630319277  \t\tvalidate 47.189555764002236\n",
      "0.125:\t#96\ttrain 111.76556692245953  \t\tvalidate 61.24555133924826\n",
      "0.125:\t#112\ttrain 136.6391698067733  \t\tvalidate 68.38989604464022\n",
      "0.125:\t#128\ttrain 138.77393496232423  \t\tvalidate 69.85327863069183\n",
      "0.125:\t#144\ttrain 138.96738964129975  \t\tvalidate 70.37182897338775\n",
      "0.125:\t#160\ttrain 138.9679627057721  \t\tvalidate 70.35616104781776\n",
      "0.125:\t#176\ttrain 138.96796669792542  \t\tvalidate 70.33907536114098\n",
      "0.125:\t#192\ttrain 138.97093791169416  \t\tvalidate 70.32725798548462\n",
      "0.125:\t#208\ttrain 141.3697542378045  \t\tvalidate 70.32191210701734\n",
      "0.125:\t#224\ttrain 142.3479081317775  \t\tvalidate 70.7668836268415\n",
      "0.125:\t#240\ttrain 142.34792840349067  \t\tvalidate 70.76718966800506\n",
      "0.125:\t#256\ttrain 145.6612170472342  \t\tvalidate 70.767148781883\n",
      "0.125:\t#272\ttrain 145.72789000651795  \t\tvalidate 70.76714833442202\n",
      "0.125:\t#288\ttrain 145.72864954404756  \t\tvalidate 70.76714836340821\n",
      "0.125:\t#304\ttrain 146.69309844183005  \t\tvalidate 71.25\n",
      "0.125:\t#320\ttrain 146.625  \t\tvalidate 71.25\n",
      "\n",
      "LAMBDA: 1\n",
      "1:\t#0\ttrain 253.92684155805682  \t\tvalidate 126.52700304112024\n",
      "1:\t#16\ttrain 125.3916362336031  \t\tvalidate 67.5139822735699\n",
      "1:\t#32\ttrain 140.23347632069974  \t\tvalidate 70.43516310802013\n",
      "1:\t#48\ttrain 140.24072105681162  \t\tvalidate 70.43083656304847\n",
      "1:\t#64\ttrain 148.625  \t\tvalidate 71.24627302268303\n",
      "1:\t#80\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#96\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#112\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#128\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#144\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#160\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#176\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#192\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#208\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#224\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#240\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#256\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#272\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#288\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#304\ttrain 148.625  \t\tvalidate 71.375\n",
      "1:\t#320\ttrain 148.625  \t\tvalidate 71.375\n",
      "\n",
      "LAMBDA: 8\n",
      "8:\t#0\ttrain 250.0396979641203  \t\tvalidate 124.77728660934494\n",
      "8:\t#16\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#32\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#48\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#64\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#80\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#96\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#112\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#128\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#144\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#160\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#176\ttrain 844.125  \t\tvalidate 424.375\n",
      "8:\t#192\ttrain 844.125  \t\tvalidate 424.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ucsc_projects\\classes\\cmps242\\hw2\\cmpshw3\\logistic_regression.py:140: RuntimeWarning: overflow encountered in multiply\n",
      "  new_weights = weights*(1-(eta*l)) - (eta*np.matmul(np.array(y_hat-labels), inputs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8:\t#208\ttrain nan  \t\tvalidate nan\n",
      "8:\t#224\ttrain nan  \t\tvalidate nan\n",
      "8:\t#240\ttrain nan  \t\tvalidate nan\n",
      "8:\t#256\ttrain nan  \t\tvalidate nan\n",
      "8:\t#272\ttrain nan  \t\tvalidate nan\n",
      "8:\t#288\ttrain nan  \t\tvalidate nan\n",
      "8:\t#304\ttrain nan  \t\tvalidate nan\n",
      "8:\t#320\ttrain nan  \t\tvalidate nan\n",
      "\n",
      "Best Values: \n",
      "\tLambda:0.000244140625\tError:10.710101796153054\n",
      "\tLambda:0.001953125\tError:11.145609911811919\n",
      "\tLambda:0.015625\tError:10.566790054302702\n",
      "\tLambda:0.125\tError:18.168692686506464\n",
      "\tLambda:1\tError:37.88205385617447\n",
      "\tLambda:8\tError:72.59097630096764\n",
      "Overall Best:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015625, 10.566790054302702]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################\n",
    "# run with the current set of lambdas to test\n",
    "########################################################\n",
    "l, t = None, None\n",
    "bests = []\n",
    "try:\n",
    "    for l in list_of_lambdas:\n",
    "        print(\"\\nLAMBDA: {}\".format(l))\n",
    "        weights = np.random.normal(0, 0.2, n_features)\n",
    "        low_val_loss = 100\n",
    "        for t in range(321):\n",
    "            weights = logistic_regression(train, train_labels, weights, l, alpha, eta_0, t)\n",
    "            val_loss = square_loss(val, val_labels, weights=weights)\n",
    "            train_loss = square_loss(train, train_labels, weights=weights)\n",
    "            if t % 16 == 0:\n",
    "                print(\"{}:\\t#{}\\ttrain {}  \\t\\tvalidate {}\".format(l, t, train_loss, val_loss))\n",
    "            if val_loss < low_val_loss:\n",
    "                low_val_loss = val_loss\n",
    "        bests.append([l, low_val_loss])\n",
    "except Exception as e:\n",
    "    print(\"\\n{} #{}: {}\".format(l, t, e), sys.stderr)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "print(\"\\nBest Values: \")\n",
    "for best in bests:\n",
    "    print(\"\\tLambda:{}\\tError:{}\".format(best[0], best[1]))\n",
    "print(\"Overall Best:\")\n",
    "min(bests, key=lambda x: x[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
