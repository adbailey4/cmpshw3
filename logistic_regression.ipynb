{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was written by Andrew Bailey and Trevor Pesout. We both contributed equally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"Module Docstrings\"\"\"\n",
    "########################################################################\n",
    "# File: template.py\n",
    "#  executable: template.py\n",
    "#\n",
    "# Author: Andrew Bailey/ Trevor Pesout\n",
    "# History: Created 10/18/17\n",
    "########################################################################\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def sigmoid(x, alpha=1, beta=0):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "\n",
    "    # this can overflow a double, so we use different implementations\n",
    "    # return math.exp(alpha*(x+beta)) / (1 + math.exp(alpha*(x+beta)))\n",
    "    if x < 0:\n",
    "        return 1 - 1 / (1 + math.exp(alpha*(x+beta)))\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-alpha*(x+beta)))\n",
    "\n",
    "\n",
    "\n",
    "def integral_of_sigmoid(x):\n",
    "    \"\"\"Returns the value from the integral of the sigmoid function\"\"\"\n",
    "    return np.log((1+np.exp(x)))\n",
    "\n",
    "\n",
    "def iterative_reweighted_least_squares(inputs, labels, weights):\n",
    "    \"\"\"Newton-Raphson iterative reweighted least squares\"\"\"\n",
    "\n",
    "\n",
    "def logistic_hessian(inputs, weights):\n",
    "    \"\"\"Calculate hessian matrix for logistic regression\"\"\"\n",
    "    y_hat = logistic_evaluation(inputs, weights)\n",
    "    b = np.array([1-y for y in y_hat])\n",
    "    # p(1-p)\n",
    "\n",
    "    prob = [y_hat*b]\n",
    "    n = len(prob[0])\n",
    "    assert n == len(y_hat)\n",
    "    d = np.repeat(prob, [n,], axis=0)\n",
    "    I = np.identity(n)\n",
    "    R = np.multiply(I, prob)\n",
    "    # hessian = np.matmul(np.matmul(inputs.T, R), inputs)\n",
    "    return R\n",
    "\n",
    "def read_spam_data(csv_f):\n",
    "    \"\"\"Read in csv of ham spam data and return two lists of data\"\"\"\n",
    "    label = []\n",
    "    message = []\n",
    "    with open(csv_f) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            message.append(row['sms'])\n",
    "            label.append(row['label'])\n",
    "            # print(row['label'], row['sms'])\n",
    "    return message, label\n",
    "\n",
    "# we want to make this use the same features with different data (so we can test)\n",
    "def create_train_data(corpus, stop_words, bigram=False, lowercase=True):\n",
    "    \"\"\"Extract features from corpus and perform the tf-idf term weighting as well as removing stopwords\"\"\"\n",
    "    # option to have 'this word' as a feature along with 'this' and 'word'\n",
    "    if bigram:\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b',\n",
    "                                     min_df=1, decode_error='ignore', stop_words=stop_words,\n",
    "                                     lowercase=lowercase)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(decode_error='ignore', stop_words=stop_words, lowercase=lowercase)\n",
    "\n",
    "    # create feature vector\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    transformer = TfidfTransformer(smooth_idf=False, norm='l2')\n",
    "    # normalize and perform tf-idf\n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    return tfidf.toarray(), vectorizer\n",
    "\n",
    "\n",
    "def create_spam_ham_labels(labels_text, spam=1, ham=0):\n",
    "    \"\"\"Convert spam and ham into either 0 or 1 \"\"\"\n",
    "    integer_labels = []\n",
    "    for label in labels_text:\n",
    "        if label == \"spam\":\n",
    "            integer_labels.append(spam)\n",
    "        else:\n",
    "            assert label == \"ham\"\n",
    "            integer_labels.append(ham)\n",
    "    return integer_labels\n",
    "\n",
    "\n",
    "def square_loss(features, labels, weights):\n",
    "    \"\"\"Calculates total loss of lists of features and labels given weights\"\"\"\n",
    "    total_loss = 0\n",
    "    for index, feature in enumerate(features):\n",
    "        a = sigmoid(np.matmul(weights, feature.T))\n",
    "        total_loss += ((a-labels[index])**2) / 2.0\n",
    "    return total_loss / len(labels)\n",
    "\n",
    "\n",
    "def logistic_loss(features, labels, weights):\n",
    "    \"\"\"Calculates total loss of lists of features and labels given weights\"\"\"\n",
    "    total_loss = 0\n",
    "    for index, feature in enumerate(features):\n",
    "        y = labels[index]\n",
    "        a = np.matmul(weights, feature.T)\n",
    "        y_hat = sigmoid(a)\n",
    "        if y_hat == 0:\n",
    "            print(\"ERROR its 0\")\n",
    "        # need to double check this equation\n",
    "        total_loss += y*np.log(y) + (1-y)*np.log(1-y_hat)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def logistic_evaluation(inputs, weights):\n",
    "    \"\"\"Returns predicted outputs from the inputs and weights\"\"\"\n",
    "    prediction = []\n",
    "    for index, feature in enumerate(inputs):\n",
    "        a = np.matmul(weights, feature.T)\n",
    "        y_hat = sigmoid(a)\n",
    "        prediction.append(y_hat)\n",
    "    return np.array(prediction)\n",
    "\n",
    "\n",
    "def logistic_regression(inputs, labels, weights, l, alpha, eta_0, step):\n",
    "    \"\"\"Implement regularized logistic regression with moving learning weight\"\"\"\n",
    "    eta = eta_0*(step**alpha)\n",
    "    y_hat = logistic_evaluation(inputs, weights)\n",
    "    new_weights = weights*(1-(eta*l)) - (eta*np.matmul(np.array(y_hat-labels), inputs))\n",
    "    return new_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 6179)\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# prepare and load the training data.  this involves\n",
    "# reading in the data and finding the best features\n",
    "########################################################\n",
    "\n",
    "# if cant find stopwords you can download using this:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# init\n",
    "stop_words = stopwords.words('english')\n",
    "train_data = 'train.csv'\n",
    "# test_data = 'test.csv'\n",
    "\n",
    "# interpret data\n",
    "messages, text_labels = read_spam_data(train_data)\n",
    "all_train_data, train_vectorizer = create_train_data(messages, stop_words)\n",
    "int_labels = create_spam_ham_labels(text_labels, spam=1, ham=0)\n",
    "\n",
    "# get sizes\n",
    "n_messages = len(messages)\n",
    "n_features = all_train_data.shape[1]\n",
    "# make sure everything is still aligned\n",
    "print(all_train_data.shape)\n",
    "assert all_train_data.shape[0] == len(messages)\n",
    "assert all_train_data.shape[0] == len(int_labels)\n",
    "assert all_train_data.shape[0] == len(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDAS:\n",
      "\tbase: 4   log_min: -6   log_max: 1\n",
      "\t[0.000244140625, 0.0009765625, 0.00390625, 0.015625, 0.0625, 0.25, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# this is the definition of the hyper parameters for\n",
    "# the regression\n",
    "########################################################\n",
    "\n",
    "# lambda\n",
    "lambda_base = 4 #8 #np.e\n",
    "lambda_exp_min = -6 #-5\n",
    "lambda_exp_max = 1 #1\n",
    "list_of_lambdas = [lambda_base**i for i in range(lambda_exp_min,lambda_exp_max+1)] \n",
    "print(\"LAMBDAS:\\n\\tbase: {}   log_min: {}   log_max: {}\\n\\t{}\".format( \n",
    "      lambda_base, lambda_exp_min, lambda_exp_max, list_of_lambdas))\n",
    "\n",
    "# sigmoid params\n",
    "eta_0 = 0.1\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# divide the data for 10-fold cross validation\n",
    "########################################################\n",
    "\n",
    "# identifiers\n",
    "TRAIN_DATA = \"t_data\"\n",
    "TRAIN_LABELS = \"t_labels\"\n",
    "VALIDATE_DATA = \"v_data\"\n",
    "VALIDATE_LABELS = \"v_labels\"\n",
    "\n",
    "# prep\n",
    "number_of_buckets = 10\n",
    "size_of_bucket = int(n_messages / number_of_buckets)\n",
    "all_train_buckets = dict()\n",
    "\n",
    "# divide into buckets\n",
    "idx = 0\n",
    "for b in range(number_of_buckets):\n",
    "    data = all_train_data[idx:idx+size_of_bucket]\n",
    "    labels = int_labels[idx:idx+size_of_bucket]\n",
    "    all_train_buckets[b] = [data, labels]\n",
    "    idx += size_of_bucket\n",
    "    \n",
    "    \n",
    "# how to create train and validation data sets\n",
    "def get_train_data_set(idx):\n",
    "    t_data, t_labels = list(), list()\n",
    "    v_data, v_labels = None, None\n",
    "    for k in all_train_buckets.keys():\n",
    "        v = all_train_buckets[k]\n",
    "        if k == idx:\n",
    "            v_data = v[0]\n",
    "            v_labels = v[1]\n",
    "        else:\n",
    "            t_data.append(v[0])\n",
    "            t_labels.append(v[1])\n",
    "    return {\n",
    "        TRAIN_DATA: np.vstack(t_data), \n",
    "        TRAIN_LABELS: np.hstack(t_labels),  \n",
    "        VALIDATE_DATA: v_data, \n",
    "        VALIDATE_LABELS: v_labels\n",
    "    }\n",
    "\n",
    "# get data\n",
    "all_training_datasets = [get_train_data_set(x) for x in list(range(number_of_buckets))]\n",
    "\n",
    "# validation\n",
    "assert len(all_training_datasets) == number_of_buckets\n",
    "for ds in all_training_datasets:\n",
    "    assert ds[TRAIN_DATA].shape[1] == n_features\n",
    "    assert ds[TRAIN_DATA].shape[0] == len(ds[TRAIN_LABELS])\n",
    "    assert ds[VALIDATE_DATA].shape[1] == n_features\n",
    "    assert ds[VALIDATE_DATA].shape[0] == len(ds[VALIDATE_LABELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# definition of our regression function\n",
    "########################################################\n",
    "\n",
    "\n",
    "def run_regression(lamda, train, train_labels, validate, validate_labels, \n",
    "                   eta_0=0.1, alpha=0.9, iterations=321, verbose=False):\n",
    "    # init\n",
    "    report_frequency = int(iterations / 16.0)\n",
    "    t = None\n",
    "    \n",
    "    #run regression\n",
    "    try:\n",
    "        weights = np.random.normal(0, 0.2, n_features)\n",
    "        min_validate_loss = sys.maxsize\n",
    "        min_train_loss = sys.maxsize\n",
    "        best_weights = None\n",
    "        for t in range(iterations):\n",
    "            weights = logistic_regression(train, train_labels, weights, lamda, alpha, eta_0, t)\n",
    "            train_loss = square_loss(train, train_labels, weights=weights)\n",
    "            val_loss = square_loss(validate, validate_labels, weights=weights)\n",
    "            if verbose and t % report_frequency == 0:\n",
    "                print(\"{}:\\t#{}\\ttrain {}  \\t\\tvalidate {}\".format(l, t, train_loss, val_loss))\n",
    "            if train_loss < min_train_loss: \n",
    "                min_train_loss = train_loss\n",
    "            if val_loss < min_validate_loss: \n",
    "                best_weights = weights\n",
    "                min_validate_loss = val_loss\n",
    "    except Exception as e:\n",
    "        print(\"\\nlambda {} #{}: {}\".format(l, t, e), sys.stderr)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # return best\n",
    "    return min_validate_loss, min_train_loss, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAMBDA: 0.000244140625\n",
      "\t.......... (357s)\n",
      "\tv errors:    [0.01205042179870902, 0.017946544821977037, 0.015083756891518361, 0.015080125864974097, 0.01733474456349715, 0.016445267105921195, 0.009715620735469504, 0.011323923592573549, 0.011118302090411293, 0.009042580495665219]\n",
      "\tv error avg: 0.013514128796071643\n",
      "\tv error std: 0.00307983600113283\n",
      "\n",
      "LAMBDA: 0.0009765625\n",
      "\t.......... (356s)\n",
      "\tv errors:    [0.01208328563336651, 0.017672249518279736, 0.014833702497650562, 0.014816646858113847, 0.017559101487407094, 0.01632975184719122, 0.010078812367188736, 0.011536310545255453, 0.011045053976121025, 0.008957130375020779]\n",
      "\tv error avg: 0.013491204510559495\n",
      "\tv error std: 0.002993912477667525\n",
      "\n",
      "LAMBDA: 0.00390625\n",
      "\t.......... (366s)\n",
      "\tv errors:    [0.012470719223168511, 0.017430335690091933, 0.014919897164791024, 0.0147432059895237, 0.017219226294959342, 0.014960532306876608, 0.010659936450595854, 0.010175532677848187, 0.009885602766859852, 0.008987132543981187]\n",
      "\tv error avg: 0.013145212110869619\n",
      "\tv error std: 0.002955056091013841\n",
      "\n",
      "LAMBDA: 0.015625\n",
      "\t.......... (355s)\n",
      "\tv errors:    [0.011062520495775205, 0.017729140734691038, 0.01550856941789295, 0.014893458391028782, 0.016828948690199675, 0.014667981339354706, 0.011697232707554734, 0.009653402576649001, 0.0061374921472281485, 0.009786627166580556]\n",
      "\tv error avg: 0.01279653736669548\n",
      "\tv error std: 0.003512014228872282\n",
      "\n",
      "LAMBDA: 0.0625\n",
      "\t.......... (349s)\n",
      "\tv errors:    [0.012522038407742036, 0.0176344827097248, 0.017173498777267968, 0.01629408711726179, 0.016664593501679353, 0.017678738897989023, 0.013233985187654868, 0.01175005348251647, 0.009700058619487974, 0.009897592348752506]\n",
      "\tv error avg: 0.014254912905007678\n",
      "\tv error std: 0.0030278609641276938\n",
      "\n",
      "LAMBDA: 0.25\n",
      "\t.......... (337s)\n",
      "\tv errors:    [0.023128471573998343, 0.02478778509957461, 0.022339361499192398, 0.021038643685772372, 0.022724892793421412, 0.022955569378878493, 0.019853044824892294, 0.020723723464724782, 0.020311020119535745, 0.01744447124647]\n",
      "\tv error avg: 0.021530698368646045\n",
      "\tv error std: 0.0019790023196532428\n",
      "\n",
      "LAMBDA: 1\n",
      "\t.......... (336s)\n",
      "\tv errors:    [0.03781535513558597, 0.0372856592480691, 0.03486612304515925, 0.033530765042007055, 0.035307738280757846, 0.035779232129238225, 0.03321170747877589, 0.03337878987133152, 0.033565626439245454, 0.030545636313240147]\n",
      "\tv error avg: 0.03452866329834105\n",
      "\tv error std: 0.0020379513150326585\n",
      "\n",
      "LAMBDA: 4\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:138: RuntimeWarning: overflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......... (351s)\n",
      "\tv errors:    [0.06289205017412895, 0.060758141074031234, 0.05915849649446896, 0.05775162675483218, 0.05952831488884558, 0.05970354871887933, 0.05333333333333334, 0.05691022963435813, 0.05820718013677701, 0.054182119968007106]\n",
      "\tv error avg: 0.05824250411776619\n",
      "\tv error std: 0.002741852502016953\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# run on each of our k-folded datasets\n",
    "########################################################\n",
    "\n",
    "# prep\n",
    "lambda_to_validate_errors = dict()\n",
    "lambda_to_training_errors = dict()\n",
    "lambda_to_weights = dict()\n",
    "\n",
    "# calculate for our lambdas\n",
    "for l in list_of_lambdas:\n",
    "    start = timer()\n",
    "    print(\"\\nLAMBDA: {}\\n\\t\".format(l),end='')\n",
    "    v_errors = list()\n",
    "    t_errors = list()\n",
    "    all_weights = list()\n",
    "    lambda_to_validate_errors[l] = v_errors\n",
    "    lambda_to_training_errors[l] = t_errors\n",
    "    lambda_to_weights[l] = all_weights\n",
    "    for dataset in all_training_datasets:\n",
    "        v_error, t_error, weights = run_regression(l, dataset[TRAIN_DATA], dataset[TRAIN_LABELS], \n",
    "                       dataset[VALIDATE_DATA], dataset[VALIDATE_LABELS])\n",
    "        v_errors.append(v_error)\n",
    "        t_errors.append(t_error)\n",
    "        all_weights.append(weights)\n",
    "        print('.', end='')\n",
    "    print(\" ({}s)\".format(int(timer() - start)))\n",
    "    print(\"\\tv errors:    {}\".format(v_errors))\n",
    "    print(\"\\tv error avg: {}\".format(np.mean(v_errors)))\n",
    "    print(\"\\tv error std: {}\".format(np.std(v_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "| log_4(lambda) |       lambda |    val error |   val stddev |  train error | train stddev |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|            -6 |     0.000244 |     0.013514 |     0.003080 |     0.000334 |     0.000045 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|            -5 |     0.000977 |     0.013491 |     0.002994 |     0.000337 |     0.000045 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|            -4 |     0.003906 |     0.013145 |     0.002955 |     0.000342 |     0.000048 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|            -3 |     0.015625 |     0.012797 |     0.003512 |     0.000410 |     0.000045 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|            -2 |     0.062500 |     0.014255 |     0.003028 |     0.001468 |     0.000104 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|            -1 |     0.250000 |     0.021531 |     0.001979 |     0.008662 |     0.000091 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|             0 |     1.000000 |     0.034529 |     0.002038 |     0.025117 |     0.000164 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|             1 |     4.000000 |     0.058243 |     0.002742 |     0.053313 |     0.000326 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# print table of data\n",
    "########################################################\n",
    "\n",
    "size = 13\n",
    "header = \"| %{}s |%{}s |%{}s |%{}s |%{}s |%{}s |\".format(size, size, size, size, size, size) % (\"log_{}(lambda)\".format(lambda_base), \"lambda\", \"val error\", \"val stddev\", \"train error\", \"train stddev\")\n",
    "spacer = \"+-\" + (\"-\" * size) + \"-+\" + (\"-\" * size) + \"-+\" + (\"-\" * size) + \"-+\" + (\"-\" * size) + \"-+\"+ (\"-\" * size) + \"-+\"+ (\"-\" * size) + \"-+\"\n",
    "line = \"| %{}d |%{}f |%{}f |%{}f |%{}f |%{}f |\".format(size, size, size, size, size, size)\n",
    "\n",
    "print(spacer)\n",
    "print(header)\n",
    "print(spacer)\n",
    "i = lambda_exp_min\n",
    "for l in list_of_lambdas:\n",
    "    print(line % (i, l, np.mean(lambda_to_validate_errors[l]), np.std(lambda_to_validate_errors[l]), np.mean(lambda_to_training_errors[l]), np.std(lambda_to_training_errors[l])))\n",
    "    print(spacer)\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwRhjTCWWGIwxxlRiicEYY0wl/x+eH1xgayrRbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dfa0b6eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################\n",
    "# analyze and print plots of the data\n",
    "########################################################\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_plot_data(items):\n",
    "    items.sort(key=lambda x: x[0])\n",
    "    x, y, var, log_lambda = list(), list(), list(), lambda_exp_min\n",
    "    for k, v in items:\n",
    "        x.append(log_lambda)\n",
    "        y.append(np.mean(v))\n",
    "        var.append(np.std(v)**2)\n",
    "        assert lambda_base**log_lambda == k\n",
    "        log_lambda += 1\n",
    "    return x, y, var\n",
    "\n",
    "validate_x, validate_y, validate_v = get_plot_data(list(lambda_to_validate_errors.items()))\n",
    "train_x, train_y, train_v = get_plot_data(list(lambda_to_training_errors.items()))\n",
    "\n",
    "plt.errorbar(train_x, train_y, train_v, None, 'bo-', label=\"Train\")\n",
    "plt.errorbar(validate_x, validate_y, validate_v, None, 'ro-', label=\"Validate\")\n",
    "plt.axis([lambda_exp_min-.5, lambda_exp_max+.5, -.005, .05])\n",
    "plt.legend(bbox_to_anchor=(.31,.95))\n",
    "plt.title(\"Logistic Regression Error Rates\")\n",
    "plt.xlabel(\"Log of Lambda (Base {})\".format(lambda_base))\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lambda: 0.015625\n",
      "Best weights have error: 0.0017970313261513231\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Get best weights\n",
    "########################################################\n",
    "\n",
    "# pick best lambda\n",
    "best_lambda = None\n",
    "min_validate_error = sys.maxsize\n",
    "for l in lambda_to_validate_errors.keys():\n",
    "    errs = lambda_to_validate_errors[l]\n",
    "    err = np.mean(errs)\n",
    "    if err < min_validate_error:\n",
    "        min_validate_error = err\n",
    "        best_lambda = l\n",
    "print(\"Best Lambda: {}\".format(best_lambda))\n",
    "        \n",
    "# test weights\n",
    "best_weights = None\n",
    "min_weight_error = sys.maxsize\n",
    "for w in lambda_to_weights[best_lambda]:\n",
    "    weight_error = square_loss(all_train_data, int_labels, weights=w)\n",
    "    if weight_error < min_weight_error:\n",
    "        min_weight_error = weight_error\n",
    "        best_weights = w\n",
    "assert best_weights is not None\n",
    "print(\"Best weights have error: {}\".format(min_weight_error))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on test data (before tf-idf transform): 0.017504640023957296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:993: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(float(n_samples) / df) + 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on test data: 0.012612299670722879\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Run on Test Data\n",
    "########################################################\n",
    "\n",
    "# get data\n",
    "test_data = 'test.csv'\n",
    "test_messages, test_text_labels = read_spam_data(test_data)\n",
    "test_labels = create_spam_ham_labels(test_text_labels, spam=1, ham=0)\n",
    "\n",
    "# extract features and test\n",
    "X_test = train_vectorizer.transform(test_messages).toarray()\n",
    "test_error = square_loss(X_test, test_labels, weights=best_weights)\n",
    "print(\"Error on test data (before tf-idf transform): {}\".format(test_error))\n",
    "\n",
    "# normalize and perform tf-idf\n",
    "transformer = TfidfTransformer(smooth_idf=False, norm='l2')\n",
    "tfidf = transformer.fit_transform(X_test)\n",
    "X_test = tfidf.toarray()\n",
    "assert X_test.shape[1] == n_features\n",
    "\n",
    "# test again\n",
    "test_error = square_loss(X_test, test_labels, weights=best_weights)\n",
    "print(\"Error on test data: {}\".format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Extra Credit: Adding in a bias term\n",
    "########################################################\n",
    "\n",
    "# because we add in the bias term\n",
    "n_features_ec = n_features + 1\n",
    "\n",
    "# create train and validation data sets with bias term\n",
    "def get_train_data_set_ec(idx):\n",
    "    t_data, t_labels = list(), list()\n",
    "    v_data, v_labels = None, None\n",
    "    for k in all_train_buckets.keys():\n",
    "        v = all_train_buckets[k]\n",
    "        if k == idx:\n",
    "            v_data = v[0]\n",
    "            v_labels = v[1]\n",
    "        else:\n",
    "            t_data.append(v[0])\n",
    "            t_labels.append(v[1])\n",
    "    \n",
    "    t_data = np.vstack(t_data)\n",
    "    t_data = np.append(np.ones([len(t_data), 1]), t_data, 1)\n",
    "    t_labels = np.hstack(t_labels)\n",
    "    v_data = np.append(np.ones([len(v_data), 1]), v_data, 1)\n",
    "    return {\n",
    "        TRAIN_DATA: t_data, \n",
    "        TRAIN_LABELS: t_labels,  \n",
    "        VALIDATE_DATA: v_data, \n",
    "        VALIDATE_LABELS: v_labels\n",
    "    }\n",
    "\n",
    "# get data\n",
    "all_training_datasets_ec = [get_train_data_set_ec(x) for x in list(range(number_of_buckets))]\n",
    "\n",
    "# validation\n",
    "assert len(all_training_datasets_ec) == number_of_buckets\n",
    "for ds in all_training_datasets_ec:\n",
    "    assert ds[TRAIN_DATA].shape[1] == n_features_ec\n",
    "    assert ds[TRAIN_DATA].shape[0] == len(ds[TRAIN_LABELS])\n",
    "    assert ds[VALIDATE_DATA].shape[1] == n_features_ec\n",
    "    assert ds[VALIDATE_DATA].shape[0] == len(ds[VALIDATE_LABELS])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Extra Credit: Function Redefinition\n",
    "########################################################\n",
    "\n",
    "def logistic_regression_ec(inputs, labels, weights, l, alpha, eta_0, step, bias=False, regularization='L2'):\n",
    "    \"\"\"Implement regularized logistic regression with moving learning weight\"\"\"\n",
    "    eta = eta_0 * (step ** alpha)\n",
    "    y_hat = logistic_evaluation(inputs, weights)\n",
    "    if regularization == 'L2':\n",
    "        if bias:\n",
    "            regularize = weights*(eta * l)\n",
    "            regularize[0] = 0\n",
    "        else:\n",
    "            regularize = weights*(eta * l)\n",
    "    elif regularization == 'L1':\n",
    "        if bias:\n",
    "            regularize = np.sign(weights)*(eta * l)\n",
    "            regularize[0] = 0\n",
    "        else:\n",
    "            regularize = np.sign(weights)*(eta * l)\n",
    "    new_weights = weights - regularize - (eta * np.matmul(np.array(y_hat-labels), inputs))\n",
    "    return new_weights\n",
    "\n",
    "def iterative_reweighted_least_squares_ec(inputs, labels, weights, lamda, bias=False):\n",
    "    \"\"\"Newton-Raphson iterative reweighted least squares\"\"\"\n",
    "    R, y_hat = logistic_hessian(inputs, weights)\n",
    "    hessian = np.matmul(np.matmul(inputs.T, R), inputs)\n",
    "    inv_hessian = np.linalg.pinv(hessian)\n",
    "    if bias:\n",
    "        regularize = weights*lamda\n",
    "        regularize[0] = 0\n",
    "    else:\n",
    "        regularize = weights*lamda\n",
    "    new_weights = weights - np.matmul(inv_hessian, np.matmul(inputs.T, (y_hat - labels)) - regularize)\n",
    "    return new_weights\n",
    "\n",
    "def run_regression_ec(lamda, train, train_labels, validate, validate_labels,\n",
    "                   eta_0=0.1, alpha=0.9, iterations=321, iwlsr=False, bias=True, regularization='L2'):\n",
    "    # init\n",
    "    report_frequency = int(iterations / 4)\n",
    "    t = None\n",
    "    # run regression\n",
    "    try:\n",
    "        weights = np.random.normal(0, 0.2, n_features_ec)\n",
    "        min_validate_loss = sys.maxsize\n",
    "        min_train_loss = sys.maxsize\n",
    "        min_weights_val_loss = []\n",
    "        for t in range(iterations):\n",
    "            if iwlsr:\n",
    "                weights = iterative_reweighted_least_squares_ec(train, train_labels, weights, lamda, bias=bias)\n",
    "            else:\n",
    "                weights = logistic_regression_ec(train, train_labels, weights, lamda, alpha, eta_0, t, bias=bias,\n",
    "                                              regularization=regularization)\n",
    "            train_loss = square_loss(train, train_labels, weights=weights)\n",
    "            val_loss = square_loss(validate, validate_labels, weights=weights)\n",
    "            if val_loss < min_validate_loss: min_weights_val_loss = weights\n",
    "            if val_loss < min_validate_loss: min_validate_loss = val_loss\n",
    "            if train_loss < min_train_loss: min_train_loss = train_loss\n",
    "    except Exception as e:\n",
    "        print(\"\\nlambda {} #{}: {}\".format(lamda, t, e), sys.stderr)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    # return best\n",
    "    return min_validate_loss, min_train_loss, min_weights_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDAS:\n",
      "\tbase: 4   log_min: -1   log_max: -1\n",
      "\t[0.25]\n",
      "\n",
      "LAMBDA: 0.25 (Extra Credit)\n",
      "\t.......... (269s)\n",
      "\tv errors:    [0.03909078819737923, 0.06833331436527516, 0.035115893852360344, 0.0633333325223199, 0.019933515332367868, 0.0699995262616341, 0.04113338896706907, 0.07668640297708201, 0.07999999998884395, 0.03454377420096687]\n",
      "\tv error avg: 0.052816993666529845\n",
      "\tv error std: 0.020021816895245292\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Extra Credit: run with bias and L2 regularization\n",
    "########################################################\n",
    "\n",
    "# my computer crashed right before this was due as we were finishing this\n",
    "# but the best result we had was at -6, so I just run that here so we\n",
    "# can get our final results\n",
    "\n",
    "# lambda\n",
    "lambda_base = 4 #8 #np.e\n",
    "lambda_exp_min = -1 #-5\n",
    "lambda_exp_max = -1 #1\n",
    "list_of_lambdas = [lambda_base**i for i in range(lambda_exp_min,lambda_exp_max+1)] \n",
    "print(\"LAMBDAS:\\n\\tbase: {}   log_min: {}   log_max: {}\\n\\t{}\".format( \n",
    "      lambda_base, lambda_exp_min, lambda_exp_max, list_of_lambdas))\n",
    "\n",
    "# sigmoid params\n",
    "eta_0 = 0.1\n",
    "alpha = 0.9\n",
    "\n",
    "\n",
    "# prep\n",
    "lambda_to_validate_errors_ec = dict()\n",
    "lambda_to_training_errors_ec = dict()\n",
    "lambda_to_weights_ec = dict()\n",
    "\n",
    "# calculate for our lambdas\n",
    "for l in list_of_lambdas:\n",
    "    start = timer()\n",
    "    print(\"\\nLAMBDA: {} (Extra Credit)\\n\\t\".format(l),end='')\n",
    "    v_errors = list()\n",
    "    t_errors = list()\n",
    "    all_weights = list()\n",
    "    lambda_to_validate_errors_ec[l] = v_errors\n",
    "    lambda_to_training_errors_ec[l] = t_errors\n",
    "    lambda_to_weights_ec[l] = all_weights\n",
    "    for dataset in all_training_datasets_ec:\n",
    "        v_error, t_error, weights = run_regression_ec(l, dataset[TRAIN_DATA], dataset[TRAIN_LABELS], \n",
    "                       dataset[VALIDATE_DATA], dataset[VALIDATE_LABELS], iwlsr=False, regularization='L2')\n",
    "        v_errors.append(v_error)\n",
    "        t_errors.append(t_error)\n",
    "        all_weights.append(weights)\n",
    "        print('.', end='')\n",
    "    print(\" ({}s)\".format(int(timer() - start)))\n",
    "    print(\"\\tv errors:    {}\".format(v_errors))\n",
    "    print(\"\\tv error avg: {}\".format(np.mean(v_errors)))\n",
    "    print(\"\\tv error std: {}\".format(np.std(v_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "| log_4(lambda) |       lambda |    val error |   val stddev |  train error | train stddev |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|            -6 |     0.000244 |     0.010718 |     0.003857 |     0.000019 |     0.000056 |\n",
      "+---------------+--------------+--------------+--------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Extra Credit: print table of data\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "size = 13\n",
    "header = \"| %{}s |%{}s |%{}s |%{}s |%{}s |%{}s |\".format(size, size, size, size, size, size) % (\"log_{}(lambda)\".format(lambda_base), \"lambda\", \"val error\", \"val stddev\", \"train error\", \"train stddev\")\n",
    "spacer = \"+-\" + (\"-\" * size) + \"-+\" + (\"-\" * size) + \"-+\" + (\"-\" * size) + \"-+\" + (\"-\" * size) + \"-+\"+ (\"-\" * size) + \"-+\"+ (\"-\" * size) + \"-+\"\n",
    "line = \"| %{}d |%{}f |%{}f |%{}f |%{}f |%{}f |\".format(size, size, size, size, size, size)\n",
    "\n",
    "print(spacer)\n",
    "print(header)\n",
    "print(spacer)\n",
    "i = lambda_exp_min\n",
    "for l in list_of_lambdas:\n",
    "    print(line % (i, l, np.mean(lambda_to_validate_errors_ec[l]), np.std(lambda_to_validate_errors_ec[l]), np.mean(lambda_to_training_errors_ec[l]), np.std(lambda_to_training_errors_ec[l])))\n",
    "    print(spacer)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Extra Credit: analyze and print plots of the data\n",
    "########################################################\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_plot_data(items):\n",
    "    items.sort(key=lambda x: x[0])\n",
    "    x, y, var, log_lambda = list(), list(), list(), lambda_exp_min\n",
    "    for k, v in items:\n",
    "        x.append(log_lambda)\n",
    "        y.append(np.mean(v))\n",
    "        var.append(np.std(v)**2)\n",
    "        assert lambda_base**log_lambda == k\n",
    "        log_lambda += 1\n",
    "    return x, y, var\n",
    "\n",
    "validate_x, validate_y, validate_v = get_plot_data(list(lambda_to_validate_errors_ec.items()))\n",
    "train_x, train_y, train_v = get_plot_data(list(lambda_to_training_errors_ec.items()))\n",
    "\n",
    "plt.errorbar(train_x, train_y, train_v, None, 'bo-', label=\"Train\")\n",
    "plt.errorbar(validate_x, validate_y, validate_v, None, 'ro-', label=\"Validate\")\n",
    "plt.axis([lambda_exp_min-.5, lambda_exp_max+.5, -.005, .05])\n",
    "plt.legend(bbox_to_anchor=(.31,.95))\n",
    "plt.title(\"Logistic Regression Error Rates (Extra Credit: bias, L2 regularization)\")\n",
    "plt.xlabel(\"Log of Lambda (Base {})\".format(lambda_base))\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lambda: 0.000244140625\n",
      "Best weights have error: 0.0010528872197246772\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Extra Credit: Get best weights\n",
    "########################################################\n",
    "\n",
    "# fix all train data\n",
    "all_train_data_ec = np.append(np.ones([len(all_train_data), 1]), all_train_data, 1)\n",
    "\n",
    "# pick best lambda\n",
    "best_lambda_ec = None\n",
    "min_validate_error_ec = sys.maxsize\n",
    "for l in lambda_to_validate_errors_ec.keys():\n",
    "    errs = lambda_to_validate_errors_ec[l]\n",
    "    err = np.mean(errs)\n",
    "    if err < min_validate_error_ec:\n",
    "        min_validate_error_ec = err\n",
    "        best_lambda_ec = l\n",
    "print(\"Best Lambda: {}\".format(best_lambda_ec))\n",
    "        \n",
    "# test weights\n",
    "best_weights_ec = None\n",
    "min_weight_error_ec = sys.maxsize\n",
    "for w in lambda_to_weights_ec[best_lambda_ec]:\n",
    "    weight_error = square_loss(all_train_data_ec, int_labels, weights=w)\n",
    "    if weight_error < min_weight_error_ec:\n",
    "        min_weight_error_ec = weight_error\n",
    "        best_weights_ec = w\n",
    "assert best_weights_ec is not None\n",
    "print(\"Best weights have error: {}\".format(min_weight_error_ec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on test data (before tf-idf transform): 0.02245931236756499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:993: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(float(n_samples) / df) + 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on test data: 0.04861773287481655\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# Extra Credit: Run on Test Data\n",
    "########################################################\n",
    "\n",
    "# get data\n",
    "test_data_ec = 'test.csv'\n",
    "test_messages_ec, test_text_labels_ec = read_spam_data(test_data_ec)\n",
    "test_labels_ec = create_spam_ham_labels(test_text_labels_ec, spam=1, ham=0)\n",
    "\n",
    "\n",
    "# extract features, add bias term, test\n",
    "X_test_ec = train_vectorizer.transform(test_messages_ec).toarray()\n",
    "X_test_ec = np.append(np.ones([len(X_test_ec), 1]), X_test_ec, 1)\n",
    "test_error_ec = square_loss(X_test_ec, test_labels_ec, weights=best_weights_ec)\n",
    "print(\"Error on test data (before tf-idf transform): {}\".format(test_error_ec))\n",
    "\n",
    "# normalize and perform tf-idf\n",
    "transformer = TfidfTransformer(smooth_idf=False, norm='l2')\n",
    "tfidf_ec = transformer.fit_transform(X_test_ec)\n",
    "X_test_ec = tfidf_ec.toarray()\n",
    "assert X_test_ec.shape[1] == n_features_ec\n",
    "\n",
    "# test again\n",
    "test_error_ec = square_loss(X_test_ec, test_labels_ec, weights=best_weights_ec)\n",
    "print(\"Error on test data: {}\".format(test_error_ec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham vs Spam: Logistic Regression\n",
    "##### Andrew Bailey and Trevor Pesout\n",
    "The task at hand is to classify email headers as either 'spam' or 'ham'. The assumtion is that there are some key words that are in the spam email headers that give them away as spam and by using logistic regression we should be able to classify the difference betwen spam and ham based on the header. \n",
    " ##### _examples:_ \n",
    " \n",
    "spam - T-Mobile customer you may now claim your FREE CAMERA PHONE upgrade & a pay & go sim card for your loyalty. Call on 0845 021 3680.Offer ends 28thFeb.T&C's apply  \n",
    "ham  -  Thanks for this hope you had a good day today   \n",
    "### Natural Language Processing \n",
    "We first needed to convert the phrases from the headers of theses emails into a format which is readable by the logistic regression algorithm. \n",
    "Some words do not add information such as 'the' and should be removed. Therefore we used the very convenient CountVectorizer class to remove stopwords from all sentences. CountVectorizer also creates an index for each word and vectorizes each input sentence into a count vector. The count vector represents every word in the sentence but it is now in a fixed length vector. In order to normalize the count vector we used TfidfTransformer to weight words by the frequency of their appearance within the entire document. \n",
    "$tf(t,d) = $ number of times a term t occurs in document d.  \n",
    "$n_d$ is the number of documents and $df(d,t)$ is how many documents have term t.\n",
    "$idf(t,d) = log \\frac{n_d}{df(d,t)}+1$ number of times a term t occurs in document d.  \n",
    "$tfidf(t,d) = tf(t,d)*idf(t,d)$\n",
    "These counts $tfidf(t,d)$ are then normalized by the Euclidean norm.\n",
    "Now that we have encoded the sentence information into a vector, we convert the ham and spam labels to 0 and 1 respectively in order to classify each sentence using the logistic regression algorithm.\n",
    "### Logistic Regression\n",
    "Now that the inputs have been vectorized, the logistic regression algorithm can be used to determine an optimized weight vector. \n",
    "Each sentence is represented by $X = [x_1, x_2, ... x_m]$  \n",
    "The label of spam or ham is represented by $y \\in (0,1)$  \n",
    "The weight vector is $\\mathbf{w} =  [w_1, w_2, ... w_m]$  \n",
    "The output before the sigmoid function is defined as $a_i = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{j=0}^{m} w_j x_j= \\mathbf{w}^TX_i$  \n",
    "Sigmoid function is $\\phi(a) = \\frac{1}{1 + e^{-a}}$  \n",
    "Final output is $\\phi(a_i) = t_i$\n",
    "Likelihood  \n",
    "$L(\\mathbf{w}) = P(\\mathbf{y} \\mid \\mathbf{X};\\mathbf{w})= \\prod_{i=1}^{n} P\\big(y^{(i)} \\mid X^{(i)}; \\mathbf{w}\\big)= \\prod^{n}_{i=1}\\bigg(\\phi\\big(a^{(i)}\\big)\\bigg)^{y^{(i)}} \\bigg(1-\\phi\\big(a^{(i)}\\big)\\bigg)^{1-y^{(i)}}$\n",
    "Regularization Terms:  \n",
    "$L2: \\frac{\\lambda}{2}\\lVert \\mathbf{w} \\lVert_2 = \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$  \n",
    "$L1: \\lambda\\lVert \\mathbf{w} \\lVert_1 = \\lambda \\sum_{j=1}^{m} |w_j|$\n",
    "Objective/Cost Function (Negative Log Likelihood + Regularization)  \n",
    "$E(w_j) = \\sum_{i=1}^{m} \\Big[ - y^{(i)} log \\bigg( \\phi\\big(a^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(a^{(i)}\\big)\\bigg)\\Big] - \\frac{\\lambda}{2} \\sum_{j=1}^{m} w_j^2$\n",
    "Minimize Negative Log Likelihood  \n",
    "$\\begin{align}\n",
    "\\nabla E(w_j) = \\frac{\\partial}{\\partial w_j} l(w_j) &= \\sum_{j=1}^{m} \\bigg[\\bigg(y \\frac{1}{\\phi(a)} - (1-y) \\frac{1}{1-\\phi{(a)}} \\bigg) \\frac{\\partial}{\\partial w_j}\\phi(a) - \\frac{\\lambda}{2} \\frac{\\partial}{\\partial w_j} w_j^2\\bigg] \\\\ \n",
    "&=- \\sum_{j=1}^{m} \\big(y - \\phi(z)\\big)x_j - \\lambda w_j\n",
    "\\end{align}$\n",
    "We now have the gradient for the weights and use a decreasing function for the step size $\\eta$ in order to decrease the size of the step with each iteration t.\n",
    "$\\begin{align}\n",
    "\\eta &= \\eta_0 * t^{-\\alpha} \\\\\n",
    "\\Delta{w_j} & = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big) - \\eta \\lambda w_j\n",
    "\\end{align}$\n",
    "We can then turn these gradients into vector representation.\n",
    "$\\begin{align}\n",
    "\\nabla E(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} l(\\mathbf{w}) &=  - \\eta\\big(\\mathbf{y} - \\mathbf{\\phi(z)}\\big)\\mathbf{X}\\big) -\\mathbf{w}\\eta \\lambda \n",
    "\\end{align}$\n",
    "##### Bias\n",
    "In order to utilize a bias term the input vector for each sample was prepended with a 1 and the weight vector was not regularized on that term. In order to accomplish this we allowed for a bias regularization option which set the regulzarizers to 0 for that term.  \n",
    "##### L1 Regularizer \n",
    "The L1 term is the sum of the absolute values of all the weights. The partial derivative with respect to the weights of the L1 term is undefined at 0 because the L1 regularizer has a slope of -1 from $-\\infty$  to 0 and of 1 from 0 to $\\infty$. Thus, when there is a weight with zero we will define the slope as zero.\n",
    "$\\begin{align}\n",
    "L1: \\lambda\\lVert \\mathbf{w} \\lVert_1 &= \\lambda \\sum_{j=1}^{m} |w_j| \\\\\n",
    "\\frac{\\partial}{\\partial w_j} L1(w_j) &=      \\begin{cases}\n",
    "       \\lambda &\\quad w_j > 0   \\\\\n",
    "       0 &\\quad w_j = 0 \\\\\n",
    "       -\\lambda &\\quad w_j < 0\\\\ \n",
    "     \\end{cases}\n",
    "\\end{align}$\n",
    "So we can replace the regularization term can be replaced so we update weights using the following equation.   \n",
    "$\\begin{align}\n",
    "sign(x) &=      \\begin{cases}\n",
    "       1 &\\quad x > 0   \\\\\n",
    "       0 &\\quad x = 0 \\\\\n",
    "       -1 &\\quad x < 0\\\\ \n",
    "     \\end{cases} \\\\ \n",
    "     \\Delta{w_j} & = - \\eta \\sum_{i=1}^{n}\\big(y^{(i)} - \\phi\\big(z^{(i)}\\big) x^{(i)} \\big) - \\eta \\lambda sign(w_j)\n",
    "\\end{align}$\n",
    "### Iterative Least Squares Regression\n",
    "The update for the iterative reweighted least squares is defined as $\\mathbf{w}^{(new)} = \\mathbf{w}^{(old)} - \\mathbf{H}^{-1}\\nabla E(\\mathbf{w})$\n",
    "$\n",
    "\\begin{align}\n",
    "\\nabla E(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} l(\\mathbf{w}) &=  - \\big(\\mathbf{y} - \\mathbf{\\phi(z)}\\big)\\mathbf{X}\\big) -\\mathbf{w}\\lambda \n",
    "\\end{align}\n",
    "$\n",
    "R is an N X N diagonal matrix with elements $R_{nn} = y_n(1-y_n)$.\n",
    "$\\begin{align}\n",
    "H &= \\nabla \\nabla E(\\mathbf{w}) = \\mathbf{X}^T \\mathbf{R} \\mathbf{X}\n",
    "\\end{align}$\n",
    "Therefore the update can be defined as:\n",
    "$\\begin{align}\n",
    "\\mathbf{w}^{(new)} = \\mathbf{w}^{(old)} - (\\mathbf{X}^T \\mathbf{R} \\mathbf{X})^{-1} (- \\big(\\mathbf{y} - \\mathbf{\\phi(z)}\\big)\\mathbf{X}\\big) -\\mathbf{w} \\lambda ))\n",
    "\\end{align}\n",
    "$\n",
    "### Analysis\n",
    "###### Bias vs Unbiased\n",
    "We analyzed the differences between including and excluding a bias term. The validation loss was significantly lower when a bias term was included. This makes sense because a bias term allows us to increase the flexibility of the model to fit the data. \n",
    "###### L1 vs L2\n",
    "We also compared the difference between L1 and L2 regularizer terms. The L1 regularizer directly pulls down all weights towards zero where the L2 regularizer tries to pull the collective squared sum of the weights to zero. There is an obvious difference in the shape of these functions but they both effictively do the same thing. However, the L1 regularizer term allows weights to eventually reach zero where the L2 regularizer does not allow for this behavior. Our results show that, with a bias term, the ######## regularizer fits better. \n",
    "###### Iterative Least Squares Regression vs Logistic Regression\n",
    "The iterative least squares regression algorithm usually requires significantly fewer iterations than the standard logistic regression algorithm. However, the computing the Hessian is very expensive and with a naive python implementation takes much longer to reach the minimum. Also, computing the Hessian requires a batch of data, so there is no natural online method using iterative least squares regression. The least squares regression algorithm also seems to overfit in only a few iterations so there is less room between iterations to find a validation loss minimum.\n",
    "We found that the steps in ILSR are much too large and overfits the training data before finding a validation local minimum comparable to logistic regression. \n",
    "### Results\n",
    "\n",
    "Unfortunately, as we were finishing our run on the extra credit from lambdas $4^{-6}$ to $4^1$, the computer we were running it on crashed.  So the final results we got for the extra credit are incomplete.  Before it crashed however, we saw that the best lambda had been at $4^{-6}$, so we ran it again with that value and finished our analysis without the plot of lambdas and errors.\n",
    "\n",
    "Looking at the results between the (original) L1 regularizer with no bias and the (extra credit) L2 regularizer with bias, we get better results on the training data for the latter parameters, and better results on the test data (0.00179 vs 0.00105) with the former parameters (0.0126 vs 0.0486).\n",
    "\n",
    "This indicates to us that the bias and L2 regularizer caused us to overfit the training data.  We would like to look into this more as we expected the modifications to improve our results, but that is beyond the timeframe of this asisgnment.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
