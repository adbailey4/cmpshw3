{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# this imports all the functions in logistic regression\n",
    "# you should be able to run this cell at any time to \n",
    "# \"reload\" the functions\n",
    "########################################################\n",
    "\n",
    "from logistic_regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 6179)\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# prepare and load the training data.  this involves\n",
    "# reading in the data and finding the best features\n",
    "########################################################\n",
    "\n",
    "# if cant find stopwords you can download using this:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# init\n",
    "stop_words = stopwords.words('english')\n",
    "train_data = 'train.csv'\n",
    "# test_data = 'test.csv'\n",
    "\n",
    "# interpret data\n",
    "messages, text_labels = read_spam_data(train_data)\n",
    "all_train_data = create_train_data(messages, stop_words)\n",
    "int_labels = create_spam_ham_labels(text_labels, spam=1, ham=0)\n",
    "\n",
    "# get sizes\n",
    "n_messages = len(messages)\n",
    "n_features = all_train_data.shape[1]\n",
    "\n",
    "# make sure everything is still aligned\n",
    "print(all_train_data.shape)\n",
    "assert all_train_data.shape[0] == len(messages)\n",
    "assert all_train_data.shape[0] == len(int_labels)\n",
    "assert all_train_data.shape[0] == len(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDAS:\n\tbase: 8   log_min: -4   log_max: 1\n\t[0.000244140625, 0.001953125, 0.015625, 0.125, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# this is the definition of the hyper parameters for\n",
    "# the regression\n",
    "########################################################\n",
    "\n",
    "# lambda\n",
    "lambda_base = 8 #np.e\n",
    "lambda_exp_min = -4\n",
    "lambda_exp_max = 1\n",
    "list_of_lambdas = [lambda_base**i for i in range(lambda_exp_min,lambda_exp_max+1)] \n",
    "print(\"LAMBDAS:\\n\\tbase: {}   log_min: {}   log_max: {}\\n\\t{}\".format( \n",
    "      lambda_base, lambda_exp_min, lambda_exp_max, list_of_lambdas))\n",
    "\n",
    "# sigmoid params\n",
    "eta_0 = 0.1\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# divide the data for 10-fold cross validation\n",
    "########################################################\n",
    "\n",
    "# identifiers\n",
    "TRAIN_DATA = \"t_data\"\n",
    "TRAIN_LABELS = \"t_labels\"\n",
    "VALIDATE_DATA = \"v_data\"\n",
    "VALIDATE_LABELS = \"v_labels\"\n",
    "\n",
    "# prep\n",
    "number_of_buckets = 10\n",
    "size_of_bucket = int(n_messages / number_of_buckets)\n",
    "all_train_buckets = dict()\n",
    "\n",
    "# divide into buckets\n",
    "idx = 0\n",
    "for b in range(number_of_buckets):\n",
    "    data = all_train_data[idx:idx+size_of_bucket]\n",
    "    labels = int_labels[idx:idx+size_of_bucket]\n",
    "    all_train_buckets[b] = [data, labels]\n",
    "    idx += size_of_bucket\n",
    "    \n",
    "    \n",
    "# how to create train and validation data sets\n",
    "def get_train_data_set(idx):\n",
    "    t_data, t_labels = list(), list()\n",
    "    v_data, v_labels = None, None\n",
    "    for k in all_train_buckets.keys():\n",
    "        v = all_train_buckets[k]\n",
    "        if k == idx:\n",
    "            v_data = v[0]\n",
    "            v_labels = v[1]\n",
    "        else:\n",
    "            t_data.append(v[0])\n",
    "            t_labels.append(v[1])\n",
    "    return {\n",
    "        TRAIN_DATA: np.vstack(t_data), \n",
    "        TRAIN_LABELS: np.hstack(t_labels),  \n",
    "        VALIDATE_DATA: v_data, \n",
    "        VALIDATE_LABELS: v_labels\n",
    "    }\n",
    "\n",
    "# get data\n",
    "all_training_datasets = [get_train_data_set(x) for x in list(range(number_of_buckets))]\n",
    "\n",
    "# validation\n",
    "assert len(all_training_datasets) == number_of_buckets\n",
    "for ds in all_training_datasets:\n",
    "    assert ds[TRAIN_DATA].shape[1] == n_features\n",
    "    assert ds[TRAIN_DATA].shape[0] == len(ds[TRAIN_LABELS])\n",
    "    assert ds[VALIDATE_DATA].shape[1] == n_features\n",
    "    assert ds[VALIDATE_DATA].shape[0] == len(ds[VALIDATE_LABELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# definition of our regression function\n",
    "########################################################\n",
    "\n",
    "\n",
    "def run_regression(lamda, train, train_labels, validate, validate_labels, \n",
    "                   eta_0=0.1, alpha=0.9, iterations=321, verbose=False):\n",
    "    # init\n",
    "    report_frequency = int(iterations / 16.0)\n",
    "    t = None\n",
    "    \n",
    "    #run regression\n",
    "    try:\n",
    "        weights = np.random.normal(0, 0.2, n_features)\n",
    "        low_val_loss = sys.maxsize\n",
    "        for t in range(iterations):\n",
    "            weights = logistic_regression(train, train_labels, weights, lamda, alpha, eta_0, t)\n",
    "            val_loss = square_loss(validate, validate_labels, weights=weights)\n",
    "            train_loss = square_loss(train, train_labels, weights=weights)\n",
    "            if verbose and t % report_frequency == 0:\n",
    "                print(\"{}:\\t#{}\\ttrain {}  \\t\\tvalidate {}\".format(l, t, train_loss, val_loss))\n",
    "            if val_loss < low_val_loss:\n",
    "                low_val_loss = val_loss\n",
    "    except Exception as e:\n",
    "        print(\"\\nlambda {} #{}: {}\".format(l, t, e), sys.stderr)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    # return best\n",
    "    return low_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# run on each of our k-folded datasets\n",
    "########################################################\n",
    "\n",
    "# prep\n",
    "lambda_to_errors = dict()\n",
    "\n",
    "# calculate for our lambdas\n",
    "for l in list_of_lambdas:\n",
    "    print(\"\\nLAMBDA: {}\".format(l))\n",
    "    errors = list()\n",
    "    for dataset in all_training_datasets:\n",
    "        e = run_regression(l, dataset[TRAIN_DATA], dataset[TRAIN_LABELS], \n",
    "                       dataset[VALIDATE_DATA], dataset[VALIDATE_LABELS])\n",
    "        errors.append(e)\n",
    "    print(\"\\terrors:    {}\".format(errors))\n",
    "    print(\"\\terror avg: {}\".format(np.mean(errors)))\n",
    "    print(\"\\terror std: {}\".format(np.std(errors)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}